	<section id="chap-create" class="chapter">
		<h2>Creation and Enrichment</h2>
		<p>In this chapter, we discuss the principal techniques by which knowledge graphs can be created and subsequently enriched from diverse sources of legacy data that may range from plain text to structured formats (and anything in between). The appropriate methodology to follow when creating a knowledge graph depends on the actors involved, the domain, the envisaged applications, the available data sources, etc. Generally speaking, however, the flexibility of knowledge graphs lends itself to starting with an initial core that can be incrementally enriched from other sources as required (typically following an Agile&nbsp;<?php echo $references->cite("HuntT03a"); ?> or “pay-as-you-go”&nbsp;<?php echo $references->cite("SequedaBMH19"); ?> methodology). For our running example, we assume that the tourism board decides to build a knowledge graph from scratch, aiming to initially describe the main tourist attractions – places, events, etc. – in Chile in order to help visiting tourists identify those that most interest them. The board decides to postpone adding further data, like transport routes, reports of crime, etc., for a later date.</p>

		<section id="sssec-graphCreationHuman" class="section">
		<h3>Human Collaboration</h3>
		<p>One approach for creating and enriching knowledge graphs is to solicit direct contributions from human editors. Such editors may be found in-house (e.g., employees of the tourist board), using crowd-sourcing platforms, through feedback mechanisms (e.g., tourists adding comments on attractions), through collaborative-editing platforms (e.g., an attractions wiki open to public edits), etc. Though human involvement incurs high costs&nbsp;<?php echo $references->cite("Paulheim18a"); ?>, some prominent knowledge graphs have been primarily based on direct contributions from human editors&nbsp;<?php echo $references->cite("VrandecicK14,LinkedInKG"); ?>. Depending on how the contributions are solicited, however, the approach has a number of key drawbacks, due primarily to human error&nbsp;<?php echo $references->cite("pellissier2016freebase"); ?>, disagreement&nbsp;<?php echo $references->cite("YasseriSRKK12"); ?>, bias&nbsp;<?php echo $references->cite("Janowicz0RZM18"); ?>, vandalism&nbsp;<?php echo $references->cite("HeindorfPSE16"); ?>, etc. Successful collaborative creation further raises challenges concerning licensing, tooling, and culture&nbsp;<?php echo $references->cite("pellissier2016freebase"); ?>. Humans are sometimes rather employed to verify and curate additions to a knowledge graph extracted by other means&nbsp;<?php echo $references->cite("pellissier2016freebase"); ?> (through, e.g., video games with a purpose&nbsp;<?php echo $references->cite("JurgensNavigli14"); ?>), to define high-quality mappings from other sources&nbsp;<?php echo $references->cite("r2rml"); ?>, to define appropriate high-level schema&nbsp;<?php echo $references->cite("onteng,Labra2017"); ?>, and so forth.</p>
		</section>

		<section id="sssec-graphCreationText" class="section">
		<h3>Text Sources</h3>
		<p>Text corpora – such as sourced from newspapers, books, scientific articles, social media, emails, web crawls, etc. – are an abundant source of rich information&nbsp;<?php echo $references->cite("HellmannLAB13,RospocherEVFARS16"); ?>. However, extracting such information with high precision and recall for the purposes of creating or enriching a knowledge graph is a non-trivial challenge. To address this, techniques from Natural Language Processing (NLP)&nbsp;<?php echo $references->cite("NLP-SW,JurafskyM18"); ?> and Information Extraction (IE)&nbsp;<?php echo $references->cite("WeikumT10,Grishman12,IESW"); ?> can be applied. Though processes vary considerably across text extraction frameworks, in Figure&nbsp;<?php echo ref("fig:textExtract"); ?> we illustrate four core tasks for text extraction on a sample sentence. We will discuss these tasks in turn.</p>

		<figure id="fig-textExtract">
			<img src="images/fig-textExtract.svg" alt="Text extraction example; dashed nodes are new to the knowledge graph"/>
			<figcaption>Text extraction example; dashed nodes are new to the knowledge graph <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_6_Creation_and_Enrichment/6_2_Text_Sources/figure_6_1.ttl"></a></figcaption>
		</figure>

		<h4 id="sssec-pre-processing" class="subsection">Pre-processing</h4>
		<p>The pre-processing task may involve applying various techniques to the input text, where Figure&nbsp;<?php echo ref("fig:textExtract"); ?> illustrates <em>Tokenisation</em>, which parses the text into atomic terms and symbols. Other pre-processing tasks applied to a text corpus may include: <em>Part-of-Speech</em> (<em>POS</em>) <em>tagging</em>&nbsp;<?php echo $references->cite("NLP-SW,JurafskyM18"); ?> to identify terms representing verbs, nouns, adjectives, etc.; <em>Dependency Parsing</em>, which extracts a grammatical tree structure for a sentence where leaf nodes indicate individual words that together form phrases (e.g., noun phrases, verb phrases) and eventually clauses and sentences&nbsp;<?php echo $references->cite("NLP-SW,JurafskyM18"); ?>; and <em>Word Sense Disambiguation</em> (<em>WSD</em>)&nbsp;<?php echo $references->cite("Navigli:09"); ?> to identify the meaning (aka <em>sense</em>) in which a word is used, linking words with a lexicon of senses (e.g., WordNet&nbsp;<?php echo $references->cite("MillerF07"); ?> or BabelNet&nbsp;<?php echo $references->cite("NavigliPonzetto:12"); ?>), where, for instance, the term <span class="tnode">flights</span> may be linked with the WordNet sense “<span class="sf">an instance of travelling by air</span>” rather than “<span class="sf">a stairway between one floor and the next</span>”. The appropriate type of pre-processing to apply often depends on the requirements of later tasks in the pipeline.</p>

		<h4 id="sssec-ner" class="subsection">Named Entity Recognition (NER)</h4>
		<p>The NER task identifies mentions of named entities in a text&nbsp;<?php echo $references->cite("NadeauS07,RatinovR09"); ?>, typically targetting mentions of people, organisations, locations, and potentially other types&nbsp;<?php echo $references->cite("LingW12,NakasholeTW13,YogatamaGL15"); ?>. A variety of NER techniques exist, with many modern approaches based on learning frameworks that leverage lexical features (e.g., POS tags, dependency parse trees, etc.) and gazetteers (e.g., lists of common first names, last names, countries, prominent businesses, etc.). Supervised methods&nbsp;<?php echo $references->cite("BikelSW99,FinkelGM05,LampleBSKD16"); ?> require manually labelling all entity mentions in a training corpus, whereas <em>bootstrapping</em>-based approaches&nbsp;<?php echo $references->cite("CollinsS99,EtzioniCDKPSSWY04,NakasholeTW13,GuptaM14"); ?> rather require a small set of <em>seed examples</em> of entity mentions from which patterns can be learnt and applied to unlabelled text. <em>Distant supervision</em>&nbsp;<?php echo $references->cite("LingW12,RenEWTVH15,YogatamaGL15"); ?> uses known entities in a knowledge graph as seed examples through which similar entities can be detected. Aside from learning-based frameworks, traditional approaches based on manually-crafted rules&nbsp;<?php echo $references->cite("KlueglAP09,ChiticariuDLRZ18"); ?> are still sometimes used due to their more controllable and predictable behaviour&nbsp;<?php echo $references->cite("ChiticariuLR13"); ?>. The named entities identified by NER may be used to generate new candidate nodes for the knowledge graph (known as <em>emerging entities</em>, shown dashed in Figure&nbsp;<?php echo ref("fig:textExtract"); ?>), or may be linked to existing nodes per the Entity Linking task described in the following.</p>

		<h4 id="sssec-el" class="subsection">Entity Linking (EL)</h4>
		<p>The EL task associates mentions of entities in a text with the existing nodes of a target knowledge graph, which may be the nucleus of a knowledge graph under creation, or an external knowledge graph&nbsp;<?php echo $references->cite("WuHH18"); ?>. In Figure&nbsp;<?php echo ref("fig:textExtract"); ?>, we assume that the nodes <span class="gnode">Santiago</span> and <span class="gnode">Easter&nbsp;Island</span> already exist in the knowledge graph (possibly extracted from other sources). EL may then link the given mentions to these nodes. The EL task presents two main challenges. First, there may be multiple ways to mention the same entity, as in the case of <span class="tnode">Rapa&nbsp;Nui</span> and <span class="tnode">Easter&nbsp;Island</span>; if we created a node <span class="gnode">Rapa&nbsp;Nui</span> to represent that mention, we would split the information available under both mentions across different nodes, where it is thus important for the target knowledge graph to capture the various aliases and multilingual labels by which one can refer to an entity&nbsp;<?php echo $references->cite("Moroetal:14"); ?>. Second, the same mention in different contexts can refer to distinct entities; for instance, <span class="tnode">Santiago</span> can refer to cities in Chile, Cuba, Spain, amongst others. The EL task thus considers a <em>disambiguation phase</em> wherein mentions are associated to candidate nodes in the knowledge graph, the candidates are ranked, and the most likely node being mentioned is chosen&nbsp;<?php echo $references->cite("WuHH18"); ?>. Context can be used in this phase; for example, if <span class="gnode">Easter&nbsp;Island</span> is a likely candidate for the corresponding mention alongside <span class="tnode">Santiago</span>, we may boost the probability that this mention refers to the Chilean capital as both candidates are located in Chile. Other heuristics for disambiguation consider a prior probability, where for example, <span class="tnode">Santiago</span> most often refers to the Chilean capital (being, e.g., the largest city with that name); centrality measures on the knowledge graph can be used for such purposes&nbsp;<?php echo $references->cite("WuHH18"); ?>.</p>

		<h4 id="sssec-er" class="subsection">Relation Extraction (RE)</h4>
		<p>The RE task extracts relations between entities in the text&nbsp;<?php echo $references->cite("ZhouSZZ05,BachB07"); ?>. The simplest case is that of extracting binary relations in a <em>closed setting</em> wherein a fixed set of relation types are considered. While traditional approaches often relied on manually-crafted patterns&nbsp;<?php echo $references->cite("Hearst92"); ?>, modern approaches rather tend to use learning-based frameworks&nbsp;<?php echo $references->cite("RollerKN18"); ?>, including supervised methods over manually-labelled examples&nbsp;<?php echo $references->cite("BunescuM05,ZhouSZZ05"); ?>. Other learning-based approaches again use bootstrapping&nbsp;<?php echo $references->cite("EtzioniCDKPSSWY04,BunescuM07"); ?> and distant supervision&nbsp;<?php echo $references->cite("MintzBSJ09,RiedelYM10,HoffmannZLZW11,SurdeanuTNM12,XuHZG13,SmirnovaC19"); ?> to forgo the need for manual labelling; the former requires a subset of manually-labelled seed examples, while the latter finds sentences in a large corpus of text mentioning pairs of entities with a known relation/edge, which are used to learn patterns for that relation. Binary RE can also be applied using unsupervised methods in an open setting – often referred to as <em>Open Information Extraction</em> (<em>OIE</em>)&nbsp;<?php echo $references->cite("BankoCSBE07,EtzioniFCSM11,FaderSE11,MausamSSBE12,Mausam16,MitchellCHTYBCM18"); ?> – whereby the set of target relations is not pre-defined but rather extracted from text based on, for example, dependency parse trees from which relations are taken.</p>
		<p>A variety of RE methods have been proposed to extract \(n\)-ary relations that capture further context for how entities are related. In Figure&nbsp;<?php echo ref("fig:textExtract"); ?>, we see how an \(n\)-ary relation captures additional temporal context, denoting when Rapa Nui was named a World Heritage site; in this case, an anonymous node is created to represent the higher-arity relation in the directed-labelled graph. Various methods for \(n\)-ary RE are based on <em>frame semantics</em>&nbsp;<?php echo $references->cite("fillmore1976frame"); ?>, which, for a given verb (e.g., “<em>named</em>”), captures the entities involved and how they may be interrelated. Resources such as FrameNet&nbsp;<?php echo $references->cite("framenet"); ?> then define frames for words, which, for example, may identify that the semantic frame for “<em>named</em>” includes a <em>speaker</em> (the person naming something), an <em>entity</em> (the thing named) and a <em>name</em>. Optional frame elements are an <em>explanation</em>, a <em>purpose</em>, a <em>place</em>, a <em>time</em>, etc., that may add context to the relation. Other RE methods are rather based on <em>Discourse Representation Theory</em> (<em>DRT</em>)&nbsp;<?php echo $references->cite("Kamp1981ATheoryOfTruth"); ?>, which considers a logical representation of text based on existential events. Under this theory, for example, the naming of Easter Island as a World Heritage Site is considered to be an (existential) event where Easter Island is the <em>patient</em> (the entity affected), leading to the logical (neo-Davidsonian) formula:</p>
		
		<p class="mathblock">\( \exists e: \big(\)naming\((e),\) patient\((e,\) <span class="tnode">Easter&nbsp;Island</span>\(),\) name\((e,\) <span class="tnode">World&nbsp;Heritage&nbsp;Site</span>\()\big) \)</p>

		<p>Such a formula is analogous to reification, as discussed previously in Section&nbsp;<?php echo ref("ssec:knowledgeContext"); ?>, where \(e\) is an existential term that refers to the \(n\)-ary relation being extracted.</p>
		<p>Finally, while relations extracted in a closed setting are typically mapped directly to a knowledge graph, relations that are extracted in an open setting may need to be aligned with the knowledge graph; for example, if an OIE process extracts a binary relation <?php echo gedge("Santiago","has&nbsp;flights&nbsp;to","Easter&nbsp;Island"); ?>, it may be the case that the knowledge graph does not have other edges labelled <span class="gelab">has&nbsp;flights&nbsp;to</span>, where alignment may rather map such a relation to the edge <?php echo gedge("Santiago","flight","Easter&nbsp;Island"); ?> assuming <span class="gelab">flight</span> is used in the knowledge graph. A variety of methods have been applied for performing such alignments, including mappings&nbsp;<?php echo $references->cite("CorcoglionitiRA16,GangemiPRNDM17"); ?> and rules&nbsp;<?php echo $references->cite("rouces2015framebase"); ?> for aligning \(n\)-ary relations; distributional and dependency-based similarities&nbsp;<?php echo $references->cite("MoroNavigli13"); ?>, association rule mining&nbsp;<?php echo $references->cite("dutta2014semantifying"); ?>, Markov clustering&nbsp;<?php echo $references->cite("Dutta2015ESKwithOI"); ?> and linguistic techniques&nbsp;<?php echo $references->cite("Martinez-Rodriguez18"); ?> for aligning OIE relations; amongst others.</p>

		<h4 id="sssec-joint-tasks" class="subsection">Joint tasks</h4>
		<p>Having presented the four main tasks for building knowledge graphs from text, it is important to note that frameworks do not always follow this particular sequence of tasks. A common trend, for example, is to combine interdependent tasks, jointly performing WSD and EL&nbsp;<?php echo $references->cite("Moroetal:14"); ?>, or NER and EL&nbsp;<?php echo $references->cite("LuoHLN15,NguyenTW16"); ?>, or NER and RE&nbsp;<?php echo $references->cite("RenWHQVJAH17,ZhengWBHZX17"); ?>, etc., in order to mutually improve the performance of multiple tasks. For further details on extracting knowledge graphs from text we refer to the book by <?php echo $references->citet("NLP-SW"); ?> and the recent survey by <?php echo $references->citet("IESW"); ?>.</p>
		</section>

		<section id="sssec-graphCreationSemistructured" class="section">
		<h3>Markup Sources</h3>
		<p>The Web was founded on interlinking <em>markup documents</em> wherein markers (aka <em>tags</em>) are used to separate elements of the document (typically for formatting purposes). Most documents on the Web use the HyperText Markup Language (HTML). Figure&nbsp;<?php echo ref("fig:html"); ?> presents an example HTML webpage about World Heritage Sites in Chile. Other formats of markup include Wikitext used by Wikipedia, TeX for typesetting, Markdown used by Content Management Systems, etc. One approach for extracting information from markup documents – in order to create and/or enrich a knowledge graph – is to strip the markers (e.g., HTML tags), leaving only plain text upon which the techniques from the previous section can be applied. However, markup can be useful for extraction purposes, where variations of the aforementioned tasks for text extraction have been adapted to exploit such markup&nbsp;<?php echo $references->cite("LuBLCG13,LockardDSE18,IESW"); ?>. We can divide extraction techniques for markup documents into three main categories: general approaches that work independently of the markup used in a particular format, often based on <em>wrappers</em> that map elements of the document to the output; focussed approaches that target specific forms of markup in a document, most typically <em>web tables</em> (but sometimes also lists, links, etc.); and form-based approaches that extract the data underlying a webpage, per the notion of the <em>Deep Web</em>. These approaches can often benefit from the regularities shared by webpages of a given website; for example, intuitively speaking, while the webpage of Figure&nbsp;<?php echo ref("fig:html"); ?> is about Chile, we will likely find pages for other countries following the same structure on the same website.</p>

		<figure id="fig-html">
			<pre style="float:left;width:60%;font-size:75%;"><code class="language-html">&lt;html&gt;
  &lt;head&gt;&lt;title&gt;UNESCO World Heritage Sites&lt;/title&gt;&lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;World Heritage Sites&lt;/h1&gt;
	&lt;h2&gt;Chile&lt;/h2&gt;
	&lt;p&gt;Chile has 6 UNESCO World Heritage Sites.&lt;/p&gt;
	&lt;table border="1"&gt;
	  &lt;tr&gt;&lt;th&gt;Place&lt;/th&gt;&lt;th&gt;Year&lt;/th&gt;&lt;th&gt;Criteria&lt;/th&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Rapa Nui&lt;/td&gt;&lt;td&gt;1995&lt;/td&gt;
		&lt;td rowspan="6"&gt;Cultural&lt;/td&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Churches of Chiloé&lt;/td&gt;&lt;td&gt;2000&lt;/td&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Historical Valparaíso&lt;/td&gt;&lt;td&gt;2003&lt;/td&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Saltpeter Works&lt;/td&gt;&lt;td&gt;2005&lt;/td&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Sewell Mining Town&lt;/td&gt;&lt;td&gt;2006&lt;/td&gt;&lt;/tr&gt;
	  &lt;tr&gt;&lt;td&gt;Qhapaq Ñan&lt;/td&gt;&lt;td&gt;2014&lt;/td&gt;&lt;/tr&gt;
	&lt;/table&gt;
  &lt;/body&gt;
&lt;/html&gt;</code></pre>
			<div style="height:3em;">&nbsp;</div>
			<div id="unesco">
				<p>UNESCO World Heritage Sites</p>
				<div>
					<div class="html-h1">World Heritage Sites</div>
					<div class="html-h2">Chile</div>
					<p>Chile has 6 UNESCO World Heritage Sites.</p>
					<table id="html-table">
						<tr><th>Place</th><th>Year</th><th>Criteria</th></tr>
						<tr><td>Rapa Nui</td><td>1995</td>
							<td rowspan="6">Cultural</td></tr>
						<tr><td>Churches of Chiloé</td><td>2000</td></tr>
						<tr><td>Historical Valparaíso</td><td>2003</td></tr>
						<tr><td>Saltpeter Works</td><td>2005</td></tr>
						<tr><td>Sewell Mining Town</td><td>2006</td></tr>
						<tr><td>Qhapaq Ñan</td><td>2014</td></tr>
					</table>
				</div>
			</div>
			<div style="height:3.6em;">&nbsp;</div>
			<figcaption>Example markup document (HTML) with source-code (left) and formatted document (right)</figcaption>
		</figure>

		<h4 id="sssec-wrapper-based-extraction" class="subsection">Wrapper-based extraction</h4>
		<p>Many general approaches are based on <em>wrappers</em> that locate and extract the useful information directly from the markup document. While the traditional approach was to define such wrappers manually – a task for which a variety of declarative languages and tools have been defined – such approaches are brittle to changes in a website’s layout&nbsp;<?php echo $references->cite("FerraraMFB14"); ?>. Hence other approaches allow for (semi-)automatically <em>inducing</em> wrappers&nbsp;<?php echo $references->cite("FlescaMM04"); ?>. A modern such approach – used to enrich knowledge graphs in systems such as LODIE&nbsp;<?php echo $references->cite("GentileZC14"); ?> – is to apply distant supervision, whereby EL is used to identify and link entities in the webpage to nodes in the knowledge graph such that paths in the markup that connect pairs of nodes for known edges can be extracted, ranked, and applied to other examples. Taking Figure&nbsp;<?php echo ref("fig:html"); ?>, for example, distant supervision may link <span class="tnode">Rapa&nbsp;Nui</span> and <span class="tnode"><strong>World&nbsp;Heritage&nbsp;Sites</strong></span> to the nodes <span class="gnode">Easter&nbsp;Island</span> and <span class="gnode">World&nbsp;Heritage&nbsp;Site</span> in the knowledge graph using EL, and given the edge <?php echo gedge("Easter&nbsp;Island","named","World&nbsp;Heritage&nbsp;Site"); ?> in the knowledge graph (extracted per Figure&nbsp;<?php echo ref("fig:textExtract"); ?>), identify the candidate path \((x,\)<span class="markupf">td</span>\([1]^{-} \cdot \) <span class="markupf">tr</span>\(^{-} \cdot \) <span class="markupf">table</span>\(^- \cdot \) <span class="markupf">h1</span>\(,y)\) as reflecting edges of the form <?php echo gedge("\\(x\\)","named","\\(y\\)"); ?>, where \(t[n]\) indicates the \(n\)<sup>th</sup> child of tag \(t\), \(t^-\) its inverse, and \(t_1 \cdot t_2\) concatenation. Finally, paths with high confidence (e.g., ones “witnessed” by many known edges in the knowledge graph) can then be used to extract novel edges, such as <?php echo gedge("Qhapaq&nbsp;Ñan","named","World&nbsp;Heritage&nbsp;Site"); ?>, both on this page and on related pages of the website with similar structure (e.g., for other countries).</p>

		<h4 id="sssec-web-table-extraction" class="subsection">Web table extraction</h4>
		<p>Other approaches target specific types of markup, most commonly <em>web tables</em> embedded in HTML webpages. However, web tables are designed to enhance human rather than machine readability. Many web tables are used for layout and page structure (e.g., navigation bars). Those that contain data may follow different formats, such as relational tables, listings, attribute-value tables, and matrices&nbsp;<?php echo $references->cite("CafarellaHWWZ08,CrestanP11"); ?>. A first step is to classify tables to find ones appropriate for the given extraction mechanism(s)&nbsp;<?php echo $references->cite("CrestanP11,EberiusBHTAL15"); ?>. Next, web tables may contain column spans, row spans, inner tables, or may be split vertically to improve human aesthetics. Table normalisation merges split tables, un-nests tables, transposes tables, etc.&nbsp;<?php echo $references->cite("PivkCSGRS07,CafarellaHWWZ08,CrestanP11,DengJLLY13,ErmilovN16,LehmbergRMB16"); ?>. Some approaches then identify the table <em>protagonist</em>&nbsp;<?php echo $references->cite("CrestanP11,MunozHM14"); ?> – the main entity that the table describes – often found elsewhere in the webpages; for example, though not mentioned by the table of Figure&nbsp;<?php echo ref("fig:textExtract"); ?>, <span class="tnode">World&nbsp;Heritage&nbsp;Sites</span> is its protagonist. Finally, extraction processes may associate cells with entities&nbsp;<?php echo $references->cite("LimayeSC10,MulwadFJ13"); ?>, columns with types&nbsp;<?php echo $references->cite("DengJLLY13,LimayeSC10,MulwadFJ13"); ?>, and column pairs with relations&nbsp;<?php echo $references->cite("LimayeSC10,MunozHM14"); ?>. When enriching knowledge graphs, recent approaches apply distant supervision, linking cells to knowledge graph nodes in order to generate candidates for type and relation extraction&nbsp;<?php echo $references->cite("LimayeSC10,MulwadFJ13,MunozHM14"); ?>. Statistical distributions can also help to link numerical columns&nbsp;<?php echo $references->cite("NeumaierUPP16"); ?>. Specialised table extraction frameworks have also been proposed for specific websites, where prominent knowledge graphs, such as DBpedia&nbsp;<?php echo $references->cite("LehmannIJJKMHMK15"); ?> and YAGO&nbsp;<?php echo $references->cite("suchanek2008yago"); ?> focus on extraction from info-box tables in Wikipedia.</p>

		<h4 id="sssec-deep-web-crawling" class="subsection">Deep Web crawling</h4>
		<p>The <em>Deep Web</em> presents a rich source of information accessible only through searches on web forms, thus requiring <em>Deep Web crawling</em> techniques to access&nbsp;<?php echo $references->cite("MadhavanKKGRH08"); ?>. Systems have been proposed to extract knowledge graphs from Deep Web sources&nbsp;<?php echo $references->cite("GellerCA08,LehmannFGNSSUBGHLA12,CollaranaG0GVA16"); ?>. Approaches typically attempt to generate sensible form inputs – which may be based on a user query or generated from reference knowledge – and then extract data from the generated responses (markup documents) using the aforementioned techniques&nbsp;<?php echo $references->cite("GellerCA08,LehmannFGNSSUBGHLA12,CollaranaG0GVA16"); ?>.</p>
		</section>

		<section id="sssec-graphCreationStructured" class="section">
		<h3>Structured Sources</h3>
		<p>Much of the legacy data available within organisations and on the Web is represented in structured formats, primarily tables – in the form of relational databases, CSV files, etc. – but also tree-structured formats such as JSON, XML etc. Unlike text and markup documents, structured sources can often be <em>mapped</em> to knowledge graphs whereby the structure is (precisely) transformed according to a mapping rather than (imprecisely) extracted. The mapping process involves two steps: 1) create a mapping from the source to a graph, and 2) use the mapping in order to materialise the source data as a graph or to virtualise the source (creating a graph view over the legacy data).</p>

		<h4 id="sssec-mapping-from-tables" class="subsection">Mapping from tables</h4>
		<p>Tabular sources of data are prevalent; for example, the structured content underlying many organisations and websites are housed in relational databases. In Figure&nbsp;<?php echo ref("fig:rdbCrime"); ?> we present an example of a relational database instance that we wish to integrate into our knowledge graph. There are then two approaches for mapping content from tables to knowledge graphs: a <em>direct mapping</em>, and a <em>custom mapping</em>.</p>

		<figure id="fig-rdbCrime">
			<div id="report">
				<p>Report</p>
				<table class="condensedTable">
					<tr>
						<th>crime</th>
						<th>claimant</th>
						<th>station</th>
						<th>date</th>
					</tr>
					<tr>
						<td>Pickpocketing</td>
						<td>XY12SDA</td>
						<td>Viña del Mar</td>
						<td>2019-04-12</td>
					</tr>
					<tr>
						<td>Assault</td>
						<td>AB9123N</td>
						<td>Arica</td>
						<td>2019-04-12</td>
					</tr>
					<tr>
						<td>Pickpocketing</td>
						<td>XY12SDA</td>
						<td>Rapa Nui</td>
						<td>2019-04-12</td>
					</tr>
					<tr>
						<td>Fraud</td>
						<td>FI92HAS</td>
						<td>Arica</td>
						<td>2019-04-13</td>
					</tr>
				</table>
			</div>
			<div style="height:1em;">&nbsp;</div>
			<div id="claimant">
				<p>Claimant</p>
				<table class="condensedTable">
					<tr>
						<th style="text-decoration:underline;">id</th>
						<th>name</th>
						<th>country</th>
					</tr>
					<tr>
						<td>XY12SDA</td>
						<td>John Smith</td>
						<td>U.S.</td>
					</tr>
					<tr>
						<td>AB9123N</td>
						<td>Jeanne Dubois</td>
						<td>France</td>
					</tr>
					<tr>
						<td>XI92HAS</td>
						<td>Jorge Hernández</td>
						<td>Chile</td>
					</tr>
				</table>
			</div>
			<div style="height:1em;">&nbsp;</div>
			<figcaption>Relational database instance with two tables describing crime data</figcaption>
		</figure>
		<figure id="fig-direct">
			<img src="images/fig-direct.svg" alt="Direct mapping result for the first rows of both tables in Figure&nbsp;33"/>
			<figcaption>Direct mapping result for the first rows of both tables in Figure&nbsp;<?php echo ref("fig:rdbCrime"); ?> <a class="git" title="Consult the code for this example on Github" href="https://github.com/Knowledge-Graphs-Book/examples/blob/main/Chapter_6_Creation_and_Enrichment/6_4_1_Mapping_from_tables/figure_6_4.ttl"></a></figcaption>
		</figure>

		<p>A direct mapping automatically generates a graph from a table. We present in Figure&nbsp;<?php echo ref("fig:direct"); ?> the result of a standard direct mapping&nbsp;<?php echo $references->cite("dm"); ?>, which creates an edge <?php echo gedge("x","y","z"); ?> for each (non-header, non-empty, non-<span class="sc">null</span>) cell of the table, such that <span class="gnode">x</span> represents the row of the cell, <span class="gelab">y</span> the column name of the cell, and <span class="gnode">z</span> the value of the cell. In particular, <span class="gnode">x</span> typically encodes the values of the primary key for a row (e.g., <strong><code>Claimant</code></strong>.<strong class="underline"><code>id</code></strong>); otherwise, if no primary key is defined (e.g., per the <strong><code>Report</code></strong> table), <span class="gnode">x</span> can be an anonymous node or a node based on the row number. The node <span class="gnode">x</span> and edge label <span class="gelab">y</span> further encode the name of the table to avoid clashes across tables that have the same column names used with different meanings. For each row <span class="gnode">x</span>, we may add a type edge based on the name of its table. The value <span class="gnode">z</span> may be mapped to datatype values in the corresponding graph model based on the source domain (e.g., a value in an SQL column of type <code>Date</code> can be mapped to <code>xsd:date</code> in the RDF data model). If the value is <span class="sc">null</span> (or empty), typically the corresponding edge will be omitted.<?php echo footnote("One might consider representing <span class=\"sc\">null</span>s with anonymous/blank nodes. However, <span class=\"sc\">null</span>s in SQL can be used to mean that there is no such value, which conflicts with the existential semantics of such nodes (e.g., in RDF)."); ?> With respect to Figure&nbsp;<?php echo ref("fig:direct"); ?>, we highlight the difference between the nodes <span class="gnode">Claimant-XY12SDA</span> and <span class="gnode">XY12SDA</span>, where the former denotes the row (or entity) identified by the latter primary key value. In case of a foreign key between two tables – such as <strong>Report.claimant</strong> referencing <strong>Claimant.<span class="underline">id</span></strong> – we can link, for example, to <span class="gnode">Claimant-XY12SDA</span> rather than <span class="gnode">XY12SDA</span>, where the former node also has the name and country of the claimant. A direct mapping along these lines has been standardised for mapping relational databases to RDF&nbsp;<?php echo $references->cite("dm"); ?>, where <?php echo $references->citet("StoicaFS19"); ?> have recently proposed an analogous direct mapping for property graphs. Another direct mapping has been defined for CSV and other tabular data&nbsp;<?php echo $references->cite("csvweb"); ?> that further allows for specifying column names, primary/foreign keys, and data types – which are often missing in such data formats – as part of the mapping itself.</p>
		<p>Although a direct mapping can be applied automatically on tabular sources of data and preserve the information of the original source – i.e., allowing a deterministic inverse mapping that reconstructs the tabular source from the output graph&nbsp;<?php echo $references->cite("SequedaAM12"); ?> – in many cases it is desirable to customise a mapping, such as to align edge labels or nodes with a knowledge graph under enrichment, etc. Along these lines, declarative mapping languages allow for manually defining custom mappings from tabular sources to graphs. A standard language along these lines is the RDB2RDF Mapping Language (R2RML)&nbsp;<?php echo $references->cite("r2rml"); ?>, which allows for mapping from individual rows of a table to one or more custom edges, with nodes and edges defined either as constants, as individual cell values, or using templates that concatenate multiple cell values from a row and static substrings into a single term; for example, a template <code>{id}-{country}</code> may produce nodes such as <span class="gnode">XY12SDA-U.S.</span> from the <strong>Claimant</strong> table. In case that the desired output edges cannot be defined from a single row, R2RML allows for (SQL) queries to generate tables from which edges can be extracted where, for example, edges such as <?php echo gedge("U.S.","crimes","2"); ?> can be generated by defining the mapping with respect to a query that joins the <strong><code>Report</code></strong> and <strong><code>Claimant</code></strong> tables on <code><strong>claimant</strong>=<strong>id</strong></code>, grouping by <code><strong>country</strong></code>, and applying a count for each country group. A mapping can then be defined on the results table such that the source node denotes the value of <code><strong>country</strong></code>, the edge label is the constant <span class="gelab">crimes</span>, and the target node is the count value. An analogous standard also exists for mapping CSV and other tabular data to RDF graphs, again allowing keys, column names, and datatypes to be chosen as part of the mapping&nbsp;<?php echo $references->cite("csvwmeta"); ?>.</p>
		<p>Once the mappings have been defined, one option is to use them to <em>materialise</em> graph data following an <em>Extract-Transform-Load</em> (<em>ETL</em>) approach, whereby the tabular data are transformed and explicitly serialised as graph data using the mapping. A second option is to use <em>virtualisation</em> through a <em>Query Rewriting</em> (<em>QR</em>) approach, whereby queries on the graph (using, e.g., SPARQL, Cypher, etc.) are translated to queries over the tabular data (typically using SQL). Comparing these two options, ETL allows the graph data to be used as if they were any other data in the knowledge graph. However, ETL requires updates to the underlying tabular data to be explicitly propagated to the knowledge graph, whereas a QR approach only maintains one copy of data to be updated. The area of <em>Ontology-Based Data Access</em> (<em>OBDA</em>)&nbsp;<?php echo $references->cite("XiaoCKLPRZ18"); ?> is concerned with QR approaches that support ontological entailments as seen in Chapter&nbsp;<?php echo ref("chap:deductive"); ?>. Although most QR approaches only support non-recursive entailments expressible as a single (non-recursive) query, some QR approaches support recursive entailments through rewritings to recursive queries&nbsp;<?php echo $references->cite("SequedaAM14"); ?>.</p>

		<h4 id="sssec-mapping-from-trees" class="subsection">Mapping from trees</h4>
		<p>A number of popular data formats are based on trees, including XML and JSON. While one could imagine – leaving aside issues such as the ordering of children in a tree – a trivial direct mapping from trees to graphs by simply creating edges of the form <?php echo gedge("\(x\)","child","\(y\)"); ?> for each node \(y\) that is a child of \(x\) in the source tree, such an approach is not typically used, as it represents the literal structure of the source data. Instead, the content of tree-structured data can be more naturally represented as a graph using a custom mapping. Along these lines, the GRDLL standard&nbsp;<?php echo $references->cite("grddl"); ?> allows for mapping from XML to (RDF) graphs, while languages such as RML allow for mapping from a variety of formats, including XML and JSON, to (RDF) graphs&nbsp;<?php echo $references->cite("DimouSSSMKW14"); ?>. In contrast, hybrid query languages such as XSPARQL&nbsp;<?php echo $references->cite("BishofDKLP12"); ?> allow for querying XML and RDF in unison, thus supporting both materialisation and virtualisation of graphs over tree-structured sources of legacy data.</p>

		<h4 id="sssec-mapping-from-other" class="subsection">Mapping from other knowledge graphs</h4>
		<p>We may also leverage existing knowledge graphs in order to construct or enrich another knowledge graph. For example, a large number of points of interest for the Chilean tourist board may be available in existing knowledge graphs such as BabelNet&nbsp;<?php echo $references->cite("NavigliPonzetto:12"); ?>, DBpedia&nbsp;<?php echo $references->cite("LehmannIJJKMHMK15"); ?>, LinkedGeoData&nbsp;<?php echo $references->cite("StadlerLHA12"); ?>, Wikidata&nbsp;<?php echo $references->cite("VrandecicK14"); ?>, YAGO&nbsp;<?php echo $references->cite("YAGO"); ?>, etc. However, not all entities and/or relations may be of interest. A standard option to extract a relevant sub-graph of data is to use construct queries that generate graphs as output&nbsp;<?php echo $references->cite("neumaier2018enabling"); ?>. Entity and schema alignment between the knowledge graphs may be further necessary to better integrate (parts of) external knowledge graphs, using linking tools for graphsexternal identifiers&nbsp;<?php echo $references->cite("pellissier2016freebase"); ?>, or indeed may be done manually&nbsp;<?php echo $references->cite("pellissier2016freebase"); ?>. For instance, Wikidata&nbsp;<?php echo $references->cite("VrandecicK14"); ?> uses Freebase&nbsp;<?php echo $references->cite("bollacker2007freebase,pellissier2016freebase"); ?> as a source; <?php echo $references->citet("gottschalk2018eventkg"); ?> extract an event-centric knowledge graph from Wikidata, DBpedia and YAGO; while <?php echo $references->citet("neumaier2018enabling"); ?> construct a spatio-temporal knowledge graph from Geonames, Wikidata, and PeriodO&nbsp;<?php echo $references->cite("GoldenS16"); ?> (as well as tabular data).</p>
		</section>

		<section id="ssec-knowledgeConceptual" class="section">
		<h3>Schema/Ontology Creation</h3>
		<p>The discussion thus far has focussed on extracting <em>data</em> from external sources in order to create and enrich a knowledge graph. In this section, we discuss some of the principal methods for generating a schema based on external sources of data, including human knowledge. For discussion on extracting a schema from the knowledge graph itself, we refer back to Section&nbsp;<?php echo ref("ssec:emergentSchema"); ?>. In general, much of the work in this area has focussed on the creation of ontologies using either ontology engineering methodologies, and/or ontology learning. We discuss these two approaches in turn.</p>

		<h4 id="sssec-ontology-engineering" class="subsection">Ontology engineering</h4>
		<p>Ontology engineering refers to the development and application of methodologies for building ontologies, proposing principled processes by which better quality ontologies can be constructed and maintained with less effort. Early methodologies&nbsp;<?php echo $references->cite("Gruninger1995,Fernandez1997,Noy2001"); ?> were often based on a waterfall-like process, where requirements and conceptualisation were fixed before starting to define the ontology, using, for example, an ontology engineering tool&nbsp;<?php echo $references->cite("gomez2006ontological,onteng,kendall2019ontology"); ?>. However, for situations involving large or ever-evolving ontologies, more iterative and agile ways of building and maintaining ontologies have been proposed.</p>
		<p>DILIGENT&nbsp;<?php echo $references->cite("Pinto2009"); ?> was an early example of an agile methodology, proposing a complete process for ontology life-cycle management and knowledge evolution, as well as separating local changes (local views on knowledge) from global updates of the core part of the ontology, using a review process to authorise the propagation of changes from the local to the global level. This methodology is similar to how, for instance, the large clinical reference terminology SNOMED&nbsp;CT&nbsp;<?php echo $references->cite("snomed2019"); ?> (also available as an ontology) is maintained and evolved, where the (international) core terminology is maintained based on global requirements, while national or local extensions to SNOMED CT are maintained based on local requirements. A group of authors then decides which national or local extensions to propagate to the core terminology. More modern agile methodologies include eXtreme Design (XD)&nbsp;<?php echo $references->cite("PresuttiDGB09,Blomqvist2016"); ?>, Modular Ontology Modelling (MOM)&nbsp;<?php echo $references->cite("hitzler2016modeling,hitzler2018tutorial"); ?>, Simplified Agile Methodology for Ontology Development (SAMOD)&nbsp;<?php echo $references->cite("peroni2016simplified"); ?>, and more besides. Such methodologies typically include two key elements: <em>ontology requirements</em> and (more recently) <em>ontology design patterns</em>.</p>
		<p>Ontology requirements specify the intended task of the resulting ontology, or of the knowledge graph itself in conjunction with the new ontology. A common way to express ontology requirements is through <em>Competency Questions</em> (<em>CQ</em>)&nbsp;<?php echo $references->cite("gruninger1995role"); ?>, which are natural language questions illustrating the typical information needs that one would require the ontology (or the knowledge graph) to respond to. Such CQs can then be complemented with additional restrictions, and reasoning requirements, in case that the ontology should also contain restrictions and general axioms for inferring new knowledge or checking data consistency. A common way of testing ontologies (or knowledge graphs based on them) is then to formalise the CQs as queries over some test set of data, and make sure the expected results are entailed&nbsp;<?php echo $references->cite("blomqvist2012ontology,keet2016test"); ?>. We may, for example, consider the CQ “<em>What are all the events happening in Santiago?</em>”, which can be represented as a graph query <span class="gnode">Event</span><?php echo etipl(); ?><span class="edge">type</span><?php echo esource(); ?><span class="gvar">?event</span><?php echo esource(); ?><span class="edge">location</span><?php echo etipr(); ?><span class="gnode">Santiago</span>. Taking the data graph of Figure&nbsp;<?php echo ref("fig:delg"); ?> and the axioms of Figure&nbsp;<?php echo ref("fig:sg"); ?>, we can check to see if the expected result <span class="gnode">EID15</span> is entailed by the ontology and the data, and since it is not, we may consider expanding the axioms to assert that <?php echo gedge("location","type","Transitive"); ?>.</p>
		<p>Ontology Design Patterns (ODPs) are another common feature of modern methodologies&nbsp;<?php echo $references->cite("gangemi2005ontology,blomqvist2005patterns"); ?>, specifying generalisable ontology modelling patterns that can be used as inspiration for modelling similar patterns, as modelling templates&nbsp;<?php echo $references->cite("Egana2008,Skjaeveland2018"); ?>, or as directly reusable components&nbsp;<?php echo $references->cite("DagaPS08,shimizu2019modl"); ?>. Several pattern libraries have been made available online, ranging from carefully curated ones&nbsp;<?php echo $references->cite("Aranguren2008,shimizu2019modl"); ?> to open and community moderated ones&nbsp;<?php echo $references->cite("DagaPS08"); ?>. As an example, to model events in our scenario, we may adopt the Core Event ontology pattern proposed by <?php echo $references->citet("KrisnadhiH16"); ?>, which specifies a spatio-temporal extent, sub-events, and participants of an event, along with competency questions, formal definitions, etc., to support this pattern.</p>

		<h4 id="sssec-ontology-learning" class="subsection">Ontology learning</h4>
		<p>The previous methodologies outline methods by which ontologies can be built and maintained manually. Ontology learning, in contrast, can be used to (semi-)automatically extract information from text that is useful for the ontology engineering process&nbsp;<?php echo $references->cite("buitelaar2005ontology,cimiano2006ontology"); ?>. Early methods focussed on extracting terminology from text that may represent the relevant domain’s classes; for example, from a collection of text documents about tourism, a terminology extraction tool – using measures of <em>unithood</em> that determine how cohesive an \(n\)-gram is as a unitary phrase, and <em>termhood</em> that determine how relevant the phrase is to a domain&nbsp;<?php echo $references->cite("Martinez-Rodriguez18"); ?> – may identify \(n\)-grams such as “<span class="sf">visitor visa</span>”, “<span class="sf">World Heritage Site</span>”, “<span class="sf">off-peak rate</span>”, etc., as terminology of particular importance to the tourist domain that thus may merit inclusion in such an ontology. Ontological axioms may also be extracted from text. A common target is to extract sub-class axioms from text, leveraging patterns based on modifying nouns and adjectives that incrementally specialise concepts (e.g., extracting <?php echo gedge("Visitor&nbsp;Visa","subc.&nbsp;of","Visa"); ?> from the noun phrase “<span class="sf">visitor visa</span>” and isolated appearances of “<span class="sf">visa</span>” elsewhere), or using Hearst patterns&nbsp;<?php echo $references->cite("Hearst92"); ?> (e.g., extracting <?php echo gedge("Off-Peak&nbsp;Rate","subc.&nbsp;of","Discount"); ?> from “<span class="sf">many <span class="underline">discounts, such as off-peak rates</span>, are available</span>” based on the pattern “<span class="sf underline">X, such as Y</span>”). Textual definitions can also be harvested from large texts to extract hypernym relations and induce a taxonomy from scratch&nbsp;<?php echo $references->cite("OntolearnReloaded13"); ?>. More recent works aim to extract more expressive axioms from text, including disjointness axioms&nbsp;<?php echo $references->cite("Volker2015"); ?>; and axioms involving the union and intersection of classes, along with existential, universal, and qualified-cardinality restrictions&nbsp;<?php echo $references->cite("petrucci2016ontology"); ?>. The results of an ontology learning process can then serve as input to a more general ontology engineering methodology, allowing us to validate the terminological coverage of an ontology, to identify new classes and axioms, etc.</p>
		</section>
	</section>
